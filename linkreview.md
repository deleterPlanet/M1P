| Название | Год | Автор | Ссылка | Краткое содержание | Релевантность |
| -------- |---- | ----- | ------ | ------------------ | ------------- |
| Gradient-based Hyperparameter Optimization through Reversible Learning | 2015 | Maclaurin, Duvenaud, Adams | [link](https://arxiv.org/pdf/1502.03492)| В статье рассматривается способ вычисления точных градиентов валидационной ошибки по гиперпараметрам, проделав дифференцирование не только через модель, но через весь процесс обучения. Таким образом гиперпараметры (например, learning rate, коэффициенты регуляризации, параметры инициализации и т.д.) становятся обучаемыми через градиент. | V |
| Optimizing Millions of Hyperparameters by Implicit Differentiation | 2019 | Lorraine, Vicol, Duvenaud | [link](https://arxiv.org/pdf/1911.02590)| Авторы масштабируют градиентную оптимизацию гиперпараметров до очень больших моделей и множества гиперпараметров, без необходимости разворачивания процесса обучения. Комбинируются Implicit Function Theorem с приближениями обратного Гессиана для вычисления гиперградентов. | V |
| Scalable Gradient-Based Tuning of Continuous Regularization Hyperparameters | 2016 | Luketina, Berglund, Greff, Raiko | [link](https://arxiv.org/pdf/1511.06727)| Предлогается “онлайн” подход, при котором гиперпараметры (коэффициенты L2, dropout, весовые коэффициенты регуляризации) могут подстраиваться во время обучения на основе градиентов валидационной ошибки, а не ждать конца обучения | X |
| Gradient-based Hyperparameter Optimization Over Long Horizons | 2021 | Micaelli, Storkey | [link](https://arxiv.org/pdf/2007.07869)| Решается проблема затухания/взрыва градиентов при долгом обучении с помощью Forward-mode Differentiation with Sharing | X |
| Differentiable Hyper-parameter Optimization | 2022 | Anonymous authors | [link](https://openreview.net/pdf?id=ROpoUxw23oP)| Оптимизация гиперпараметров через дифференцирование. Градиенты гиперпараметров получаются с помощью автоматического дифференцирования | V |
| Differentiable Model Selection for Ensemble Learning | 2023 | Kotary, Di Vito, Fioretto | [link](https://www.ijcai.org/proceedings/2023/0217.pdf)| Рассматривается задача агрегации уже обученных моделей. Для этого параметризуют агрегирующую функцию с помощью нейросети | X |
| Hyperboost: Hyperparameter Optimization by Gradient Boosting surrogate models | 2021 | Hoof, Vanschoren | [link](https://arxiv.org/pdf/2101.02289)| авторы используют градиентный бустинг как суррогатную модель в процессе оптимизации гиперпараметров | X |
| Generic Methods for Optimization-Based Modeling | 2012 | Domke | [link](https://proceedings.mlr.press/v22/domke12/domke12.pdf)| Рассматриваются общие методы дифференцирования сквозь процедуры оптимизации для обучения гиперпараметров | V |
| Forward and Reverse Gradient-Based Hyperparameter Optimization | 2017 | Franceschi, Donini, Frasconi, Pontil | [link](https://arxiv.org/pdf/1703.01785)| Рассматривается прямое и обратное автомотическое дифференцирование для обучения гиперпараметров. Показана возможность обучения сотен гиперпараметров | V |
| Truncated Back-propagation for Bilevel Optimization | 2019 | Shaban, Cheng, Hatch, Boots | [link](https://arxiv.org/pdf/1810.10667)| Разработали приближённый метод для обучения гиперпараметров через усечённый backprop. Компромисс между точностью и вычислительной эффективностью | V |
| Scalable Gradient-Based Tuning of Continuous Regularization Hyperparameters | 2016 | Luketina, Berglund, Greff, Raiko | [link](https://arxiv.org/pdf/1511.06727)| Рассматривается оптимизация сотен/тысяч гиперпараметров регуляризации | X |
| Efficient and Modular Implicit Differentiation | 2022 | Blondel, Berhet, Cuturi, ... | [link](https://arxiv.org/pdf/2105.15183)| Систематизация подходов на базе неявного дифференцирования. Предложили фрейворк | X |
| Implicit Differentiation for Fast Hyperparameter Selection in Non-Smooth Convex Learning | 2022 | Bertrand, Klopfenstein, Massias, Blondel, Vaiter | [link](https://arxiv.org/pdf/2105.01637)| Рассматривается случай негладких задач и показывается, как применять неявное дифференцирование даже при недифференцируемых функциях. Получили быстрый метод подбора гиперпараметров, применимый к задачам с L1-регуляризацией | X |
| Automatic differentiation of algorithms for machine learning | 2014 | Baydin, Pearlmutter | [link](https://arxiv.org/pdf/1404.7456)| Обзор методов автоматического дифференцирования | X |
| Beyond backpropagation: bilevel optimization through implicit differentiation and equilibrium propagation | 2022 | Zucchet, Sacramento | [link](https://arxiv.org/pdf/2205.03076)| Обучение гиперпараметров на основе неявного дифференцирования и equilibrium propagation | X |
| Principled and Efficient Bilevel Optimization for Machine Learning | 2023 | Grazzi | [link](https://discovery.ucl.ac.uk/id/eprint/10178090/1/PhD_Thesis_RG.pdf)| Постановка задачи Bilevel Optimization и рассмотрение методов её решения | X |
| Bilevel Optimization: Convergence Analysis and Enhanced Design | 2021 | Ji, Yang, Liang | [link](https://arxiv.org/pdf/2010.07962)| Анализ теоретической сходимости Bilevel алгоритмов. Введение доработок для гарантии сходимости | X |
| Bilevel Optimization Made Easy: A Simple First-Order Approach | 2022 | Ye, Liu, Wright, Stone, Liu | [link](https://arxiv.org/pdf/2209.08709)| Предлагается оптимизированный метод первого порядка для Bilevel Optimization  | X |
| On Finding Small Hyper-Gradients in Bilevel Optimization | 2025 | Chen, Xu, Zhang | [link](https://arxiv.org/pdf/2301.00712)| Рассматривается проблема маленьких гипер-градиентов в Bilevel Optimization. Приводят корректировки для избежания затухания градиента | X |
| Analyzing Inexact Hypergradients for Bilevel Learning | 2023 | Ehrhardt, Roberts | [link](https://arxiv.org/pdf/2301.04764)| Рассматриваются гипер-градиенты, вычисленные приближённо. Даются критерии, при которых такие гипер-градиенты дают сходимость | X |
| Accelerating Inexact HyperGradient Descent for Bilevel Optimization | 2023 | Yang,, Luo, Li, Jordan | [link](https://arxiv.org/pdf/2307.00126)| Предлогаются методы ускорения спуска с неточными гипер-градиентами | X |
| Riemannian Bilevel Optimization | 2024 | Dutta, Cheng, Sra | [link](https://arxiv.org/pdf/2405.15816)| Расширяют Bilevel Optimization на римонановы многообразия. Разрабатывается теория и алгоритмы | X |
| Enhancing Hypergradients Estimation: A Study of Preconditioning and Reparameterization | 2024 | Ye, Peyre, Cremers, Ablin | [link](https://arxiv.org/pdf/2402.16748)| Исследуется, как предобусловливание и репараметризация влияет на Bilevel Optimization. Показано, что подходящие преобразования могут существенно ускорить сходимость и повысить устойчивость алгоритмов | X |
| FedNest: Federated Bilevel, Minimax, and Compositional Optimization | 2022 | Tarzanagh, Li, Thrampoulidis, Oymak | [link](https://arxiv.org/pdf/2205.02215)| Представляется метод FedNest, который позволяет вычислять гиперградиенты и обновлять параметры модели в распределённой среде без вложенных циклов и полной синхронизации  | X |
| A Single-Loop Algorithm for Decentralized Bilevel Optimization | 2024 | Dong, Ma, Yang, Yin | [link](https://arxiv.org/pdf/2311.08945)| Предлагается одно-петлевой алгоритм для Bilevel Optimization в распределённых сетях, в котором нет внутренней итерации на каждом узле | X |
| Sparse MoEs meet Efficient Ensembles | 2023 | Allingham, Wenzel, Mariet, Mustafa, ... | [link](https://arxiv.org/pdf/2110.03360)| Кобинация разреженной смеси экспертов и ансамблей для достижения баланса между качеством и скоростью | V |
| Gradient-based Bi-level Optimization for Deep Learning: A Survey | 2023 | Chen, Ma, Liu | [link](https://arxiv.org/pdf/2207.11719)| Обзор методов градиентной Bilevel Optimization в DL | X |
| Ensemble learning: A survey | 2018 | Sagi, Rokach | [link](https://cris.bgu.ac.il/en/publications/ensemble-learning-a-survey)| Широкий обзор ансамблевых методов | V |
| A Comprehensive Survey of Mixture-of-Experts: Algorithms, Theory, and Applications | 2025 | Mu, Lin | [link](https://arxiv.org/pdf/2503.07137v1)| Обзор MoE | V |
| Learning Factored Representations in a Deep Mixture of Experts | 2014 | Eigen, Ranzato, Sutskever | [link](https://arxiv.org/pdf/1312.4314)| Рассматривается метод факторизации параметров в MoE для сокращения вычислительных затрат | V |
| Learning Aggregation Functions | 2021 | Pellegrini, Tibo, Frasconi, Passerini, Jaeger | [link](https://arxiv.org/pdf/2012.08482)| Введение обучаемых агрегирующих функций | V |
| A Survey on Mixture of Experts | 2024 | Cai, Jiang, Wang, Tang, Kim, Huang | [link](https://arxiv.org/pdf/2407.06204v1)| Общий обзор MoE | V |
| Choquet Fuzzy Integral based Modeling of Nonlinear System | - | Srivastava, Singh, Madasu, Hanmandlu | [link](https://eprints.qut.edu.au/12746/1/ASOC-choquet-accepted.pdf)| Рассматривается агрегация с взаимодействием моделей через Choquet / fuzzy интегралы. В итоге получили нелинейную, обучаемую агрегацию | V |
| Learning to Aggregate Using Uninorms | - | Melnikov, Hullermeier | [link](https://cs.uni-paderborn.de/fileadmin-eim/informatik/fg/is/Publications/ECML16.pdf)| Предлагают параметризованные агрегирующие функции на основе uninorms и показывают, как их обучать на данных | V |
| A Comparison of Methods for Neural Network Aggregation | 2023 | Segev | [link](https://arxiv.org/pdf/2303.03488v1)| Сравнение различных методов агрегирования нейросетевых предсказаний | V |
| Deep Mixture of Experts via Shallow Embedding | 2019 | Wang, Yu, Dunlap, Ma, Wang | [link](https://arxiv.org/pdf/1806.01531)| Представляется Deep MoE | V |
| Constructing multi-layer classifier ensembles using generalized Choquet integrals | 2022 | Batista, Bedregal, Moraes | [link](Constructing multi-layer classifier ensembles using generalized Choquet integrals)| Предлагают многослойные ансамбли с агрегирующими функциями на основе Choquet-интегралов для улучшения взаимодействия между базовыми классификаторами | V |
| Aggregation functions based on the Choquet integral applied to image resizing | 2019 | Bueno, Dias, Dimuro, Santos, Borges, Lucca, Bustince | [link](https://www.semanticscholar.org/paper/Aggregation-functions-based-on-the-Choquet-integral-Bueno-Dias/a4b27d9ff0741e80e5e7511cd977d14b9926f215)| Предлагают новые семейства агрегирующих функций, вдохновлённые Choquet-интегралом, с возможностью обучения весов взаимодействия между компонентами | V |
| FedAWA: Adaptive Optimization of Aggregation Weights in Federated Learning | 2025 | Shi, Zhao, Zhang, Zhou, Guo, ... | [link](https://arxiv.org/pdf/2503.15842)| Разработан метод адаптивной оптимизации весов агрегирования в федеративном обучении, где данные распределены по клиентам | X |
| Ensembling Diffusion Models via Adaptive Feature Aggregation | 2024 | Wang, Tian, Guan, Zhang, Jiang, Shen, Han, Gu, Yang | [link](https://arxiv.org/pdf/2405.17082v1)| Применяют адаптивную агрегацию признаков для объединения нескольких диффузионных моделей | X |
| Stacking / Stacked Generalization | 1992 | Wolpert | [link](https://machine-learning.martinsewell.com/ensembles/stacking/Wolpert1992.pdf)| Просто stacking | V |
| Artificial intelligence for load forecasting: A stacking approach | 2022 | Shi, Li, Yan | [link](https://download.ssrn.com/egy/595ff698-0cd0-47fa-9e3c-98c530470c2b-meca.pdf?response-content-disposition=inline&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEPL%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQCZngQ8QIWU67z0zKHmuth1xYkOqm3VQXTlC5NrZWO%2BRgIgEFsnod6Xk%2F2fkCBmwy1r%2B6hLVqwqvIjSrJV2svvFdFMqvAUIexAEGgwzMDg0NzUzMDEyNTciDJU%2FODt7n7%2FmITSwFSqZBX5Sd5AfarNoCxZkbix4GAkJSSBLNp6Pfi%2B4ASjaalQAN3MFCrGFr74Yv2KRgd2rzaD5RCCWk8argImtTb84ZTo2XhxHUsjKp1zmWlCdOz6%2BvyyEhWLTHJm8kJPKegq85tSw%2FMXEKmeQdVEWDbGgsY8v7nMI46PLjEDEm1UgBiRzc%2BCmU31X9nfOEMmdV%2FoQ4HDsz5IBmJjIo0mECmQZ5bfIqIIuTEpym7q2TEt0vpknVuVO1yt%2BqP9T%2BecNWGWim4xfVM7yda5t3CMF9ly28gpOI%2BvO9bIwNlZB60GeaE%2BY1cdjgSJbWmjKlCQlfb2ZyERnTe8Hk26XpilU9zPXoU%2FiuNfjrON9CTlSFdi3teLNxKrN1qmJgebrvNS6%2FJyDMFJ%2BlhZ0LrY9h2FK2kfkCuB8jejLr8sdx9nbryUmVsb9Bcywbv6KOTO%2Fzm666hHm%2Fm2DZqepo6TJTCExAyKsKDfgPiCy3CmgSgko47qOEuj1UtaJqT5%2F1z%2F%2Bz4kM60PtwbMrMi%2BzZlUp6eZxN5uclqFfQVEuJ4Fit52Zweeiz%2FB8a5LifcVtQuWAhXrqXgKAZmVTz40LDGr2M34LcCqc3Sgt1aNDUdjKbNpttwfScx%2FJFYOaQ%2Fe%2FpX8iI6WSQTlvv0NHF0pO8q%2Fy6iWxbSwVej4YLniEswsRIfVOkblVQOXfyHMPP2vuUNlyDfCZaNjp5L8JMITEGbhBRNdaCPlOUMRsksMhBnXr6IgvIaX%2BznLAPGbT6LjNndU5uYTVLxf11yl%2FrooQ7At03atdtRzA%2BAmPZtZoiESHW5rheyKyW2lz%2FQKiKVIpR0tuUhng4E6QMylVqInuFSo%2FBw51edycAEZszYMXiERZt2qunzfHYDtEf2BodgEcL86LMK731cYGOrEBPKAZzFOm2q5E9KHLNHiXVw21BvqGdrnt6BUu6oNlScZ2gSLMEja%2Bh%2FwFEZxZFzX9fru5amfKr3opsEVYVgYMkupMor8OQF9CxW4S%2Fz4U%2FF1BxY05Pwx0HPjBkdXI5n08zAJOwycsnyNuLt8q7IgAcsC1E4J0r6s10xl4Ud0faOar5r5JgLzDgjXNiU0AetFqKVWAeN0VnFRsHnNib6NN5LBU0gkmIQaGMdViyZmjanJ0&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250925T174812Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAUPUUPRWEYPDRIXJU%2F20250925%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=13666f1e814c3f9e65d7399759c4b6f8268fb0bb1205db5bcfc410bb5b1bca0a&abstractId=3979977)| Применили stacking для прогнозирования энергопотребления с использованием разнообразных базовых моделей | X |
