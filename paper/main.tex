\documentclass{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}



\title{Оптимизация количества деревьев в ансамбле градиентного бустинга с использованием стратегий выбора агрегирующих функций}

\author{ Якушевич Антон Сергеевич \\
	ВМК \\
	МГУ\\
	Москва \\
	\texttt{s02220301@gse.cs.msu.ru} \\
}
\date{}

\renewcommand{\shorttitle}{M1P}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={Оптимизация количества деревьев в ансамбле градиентного бустинга с использованием стратегий выбора агрегирующих функций},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={Якушевич Антон Сергеевич},
pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\maketitle

\begin{abstract}
	В работе исследуется задача оптимизации ансамблей градиентного бустинга за счёт выбора обобщённой агрегирующей функции. Исследование проводится для уменьшения количества деревьев в ансамбле с целью повышения интерпретируемости модели при сохранении её прогностической точности. Для этого предлагается использовать усечённое разложение агрегирующей функции в ряд Тейлора и оптимизировать её коэффициенты с помощью градиентного спуска, что позволяет адаптивно настраивать структуру ансамбля и улучшать баланс между точностью и интерпретируемостью. Экспериментальные результаты показывают, что оптимизация коэффициентов ряда Тейлора приводит к уменьшению числа деревьев без потери качества, а в ряде случаев даже к улучшению точности
\end{abstract}


\keywords{First keyword \and Second keyword \and More}

\section{Введение}
% Зачем это надо
Градиентный бустинг остаётся одним из наиболее эффективных методов машинного обучения, применяемых в задачах регрессии и классификации. Однако высокая точность ансамбля достигается ценой увеличения числа деревьев, что усложняет интерпретацию модели и приводит к увеличению вычислительных затрат. Поэтому важной задачей является разработка подходов, позволяющих уменьшить размер ансамбля без ухудшения качества прогнозов.

% Как это решать (краткий обзор литературы)
Одним из направлений снижения сложности ансамблей является модификация агрегирующей функции. Наиболее простой подход заключается во введении весов для отдельных деревьев, которые подбираются либо вручную, либо оптимизируются на валидационных данных \citep{domke2012generic}. Более сложные методы рассматривают автоматическую настройку весов или коэффициентов, что позволяет повысить адаптивность ансамбля \citep{franceschi2017forward}. Другая линия исследований связана с использованием мета-моделей для агрегирования, таких как нейронные сети, которые применяются для того, чтобы обучаться более сложным функциям взвешивания базовых предсказаний \citep{differentiable2022hyperopt}. Подобные подходы расширяют пространство возможных агрегирующих функций и потенциально позволяют достичь более высокой точности. В смежных направлениях рассматривались и дифференцируемые методы оптимизации гиперпараметров \citep{maclaurin2015gradient, lorraine2019optimizing, shaban2019truncated}, где подбираются параметры регуляризации и обучения, что демонстрирует применимость автоматического дифференцирования к задачам настройки сложных моделей.

% Какие проблемы у решений
Несмотря на успехи, существующие подходы имеют ряд ограничений. Во-первых, во многих работах агрегирующая функция фиксируется заранее (например, простое суммирование или линейная комбинация), что ограничивает адаптивность ансамбля \citep{domke2012generic}. Во-вторых, даже в случаях, когда веса подбираются автоматически, процесс оптимизации часто является дорогостоящим и требует перебора гиперпараметров или применения эвристик \citep{franceschi2017forward}. В-третьих, подходы, где агрегирующая функция параметризуется нейронной сетью \citep{differentiable2022hyperopt}, сталкиваются с ограничением: обучение функции проводится поверх уже зафиксированного набора деревьев, что не позволяет агрегирующей функции влиять на сам процесс построения ансамбля. Это делает такие методы менее гибкими и усложняет их использование на практике.

% Предлагаемое решение
В данной работе агрегирующая функция ансамбля представляется в виде усечённого разложения в ряд Тейлора, где коэффициенты ряда рассматриваются как параметры, подлежащие обучению. Для оптимизации этих коэффициентов используется следующая идея. Сначала строится оператор, который принимает на вход набор коэффициентов, затем обучает ансамбль деревьев с фиксированными гиперпараметрами, и, наконец, вычисляет значение функции потерь на валидационных данных. Возвращаемое оператором значение служит целевой функцией для обновления коэффициентов. Таким образом, коэффициенты агрегирующей функции оптимизируются напрямую с помощью градиентного спуска. Благодаря этому достигается совместная настройка структуры ансамбля и параметров агрегирования, что даёт дополнительную гибкость по сравнению с фиксированными или отдельно обучаемыми функциями.

% Результаты и вклад
Предложенный метод открывает новый способ интеграции оптимизации агрегирующих функций в процесс обучения ансамблей. В отличие от подходов, где агрегирующая функция задаётся заранее или обучается постфактум, наша методика позволяет адаптивно корректировать её форму одновременно с построением деревьев. Экспериментальные результаты показывают, что оптимизация коэффициентов ряда Тейлора приводит к уменьшению числа деревьев без потери качества, а в ряде случаев даже к улучшению точности. Тем самым достигается более выгодный баланс между точностью, интерпретируемостью и вычислительной эффективностью. Вклад работы заключается в том, что она расширяет область применения методов оптимизации, показывая, что обучение агрегирующих функций может быть встроено непосредственно в процесс градиентного бустинга, а не рассматриваться как внешний этап.


\section{Литературный обзор}
Ансамблевые методы давно зарекомендовали себя как один из наиболее эффективных классов алгоритмов машинного обучения \citep{breiman1996bagging, freund1997adaboost}. Среди них особое место занимает градиентный бустинг \citep{friedman2001greedy}, который благодаря высокой точности и универсальности получил широкое распространение как в академических исследованиях, так и в индустриальных приложениях \citep{chen2016xgboost, ke2017lightgbm}. Однако рост числа деревьев в ансамбле приводит к ухудшению интерпретируемости и повышению вычислительных затрат, что стимулировало поиск методов сокращения сложности без потери качества.

Одним из направлений развития бустинга стали методы регуляризации и усечения ансамблей. Ряд работ показал эффективность ограничений на глубину деревьев и скорости обучения \citep{zhang2005boosting, buhlmann2007boosting}, а также введения различных форм регуляризации \citep{mason2000boosting, friedman2002stochastic}. Другой класс подходов связан с сокращением ансамбля путём отбора наиболее информативных моделей \citep{margineantu1997reduced, caruana2004ensemble}, что позволяет уменьшить число деревьев при минимальных потерях в точности.

Ключевым элементом ансамблевых методов является агрегирующая функция, которая объединяет предсказания отдельных базовых алгоритмов в итоговый результат. В классических вариантах бустинга используется простая сумма или усреднение выходов деревьев с фиксированными весами \citep{hastie2009elements}. Однако такая схема имеет ограниченную гибкость и не всегда позволяет учесть различный вклад деревьев в итоговую точность. Для повышения адаптивности были предложены методы взвешивания, где каждому дереву приписывается коэффициент, подбираемый либо с помощью оптимизации на валидационных данных, либо через регуляризацию \citep{domke2012generic, franceschi2017forward}. Подобные подходы позволяют усилить значимость наиболее полезных моделей и, наоборот, снизить влияние переобученных деревьев.

В смежных исследованиях развивались методы stacking, где агрегирующая функция задавалась в виде отдельной модели, обучаемой на предсказаниях базовых алгоритмов \citep{wolpert1992stacked, ting1999issues, maclin1999popular}. В простейшем случае это линейная или логистическая регрессия, а в более сложных вариантах — регуляризованные модели или мета-классификаторы, способные учитывать взаимосвязи между деревьями. Такие методы обеспечивали более богатое пространство комбинаций по сравнению с фиксированными весами, но усложняли интерпретацию и повышали риск переобучения.

В последние годы начали активно применяться подходы, где агрегирующая функция параметризуется нейронными сетями \citep{cortes2017adanet, differentiable2022hyperopt}. Нейросетевой агрегатор способен моделировать нелинейные зависимости между выходами деревьев и строить адаптивные правила комбинирования, выходящие за рамки линейных комбинаций. Это открывает возможность учёта сложных взаимодействий внутри ансамбля. Однако ключевым ограничением таких методов является то, что обучение агрегирующей функции проводится поверх уже построенного ансамбля деревьев: деревья фиксируются, и нейросеть лишь учится их комбинировать. В результате агрегатор не влияет на сам процесс формирования ансамбля, что снижает потенциал такого подхода.

Параллельно активно развивалось направление дифференцируемой оптимизации гиперпараметров. Ключевая идея была предложена в работах \citet{maclaurin2015gradient} и \citet{domke2012generic}, где рассматривалось дифференцирование процесса обучения для вычисления градиентов валидационной ошибки. Дальнейшие исследования сосредоточились на масштабировании этого подхода \citep{franceschi2017forward, lorraine2019optimizing, shaban2019truncated}, использовании неявного дифференцирования и аппроксимаций обратного Гессиана \citep{pedregosa2016hyperparameter}, а также применении автоматического дифференцирования в широком классе моделей \citep{baydin2018automatic}. Эти работы подтвердили возможность точной и вычислительно эффективной оптимизации параметров, выходящих за рамки традиционных методов перебора и байесовской оптимизации \citep{snoek2012practical, bergstra2011algorithms}.

Несмотря на прогресс, существующие методы имеют ограничения. Весовые и линейные комбинации деревьев не всегда обеспечивают достаточную гибкость, а нейросетевые агрегаторы обучаются поверх фиксированного ансамбля, не влияя на сам процесс построения деревьев. Дифференцируемые методы гипероптимизации сосредоточены преимущественно на настройке параметров обучения и регуляризации, а не на структурных характеристиках агрегирования. Эти ограничения формируют исследовательский разрыв, восполняемый настоящей работой, где агрегирующая функция представляется в виде усечённого разложения в ряд Тейлора, а её коэффициенты оптимизируются напрямую градиентным спуском в ходе построения ансамбля.


\bibliographystyle{unsrtnat}
\bibliography{references}

\end{document}